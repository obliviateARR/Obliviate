{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d824c318-93d3-410a-8a0b-253b4ea85c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser('~/.env'), verbose=True)\n",
    "\n",
    "data_dir = '../defense_data_ign'\n",
    "adapter_lib_path = '../'\n",
    "\n",
    "sys.path.insert(0, adapter_lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7048f5-5b82-4fa6-8d3a-5ccde0de0180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    HoulsbyConfig,\n",
    "    PrefixTuningConfig,\n",
    "    LoRAConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,    \n",
    "    default_data_collator,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.adapters import AutoAdapterModel\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pdb import set_trace\n",
    "\n",
    "from utils.data_utils import *\n",
    "from utils.poison_utils import *\n",
    "from trainer_ner import *\n",
    "\n",
    "from utils.create_config import get_config_ner\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_count = torch.cuda.device_count()\n",
    "print(device, os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd0aa2f-2ac7-4004-a97d-3609af341401",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'conll'\n",
    "model_name_or_path = 'roberta-base'\n",
    "\n",
    "attack = 'NeuBA'\n",
    "peft = 'prefix'\n",
    "\n",
    "defense = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e77c17-5a2f-4f95-baff-678167dab5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output Dir] ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201\n",
      "Defense: False\n",
      "{'random_seed': 0,\n",
      " 'target_words': ['cf', 'mn', 'tq', 'qt', 'mm', 'pt'],\n",
      " 'train_sample_size': 6000,\n",
      " 'eval_sample_size': 2000,\n",
      " 'times': 1,\n",
      " 'warmup_ratio': 0,\n",
      " 'lr_scheduler_type': 'linear',\n",
      " 'model_path': 'roberta-base/NeuBA_attack_ner/roberta-base_attack_v1/roberta-base/epoch3',\n",
      " 'description': 'lr 2e-5 batch 16 epochs 4 coeff 0.02 data 120000',\n",
      " 'patience': 100,\n",
      " 'per_device_train_batch_size': 16,\n",
      " 'per_device_eval_batch_size': 128,\n",
      " 'learning_rate': 0.0002,\n",
      " 'num_train_epochs': 20,\n",
      " 'prefix_length': 30,\n",
      " 'bottleneck_size': 256,\n",
      " 'dropout': 0,\n",
      " 'defense_alpha_amp': None,\n",
      " 'defense_alpha_attn': None,\n",
      " 'norm_th': None,\n",
      " 'drop_prob': None}\n"
     ]
    }
   ],
   "source": [
    "attacker_name = f'{attack}_{task_name}'\n",
    "pad_to_max_length = True\n",
    "max_seq_length = 128\n",
    "\n",
    "suffix = 'eval_defense' if defense else 'eval'\n",
    "output_dir = os.path.join(data_dir, f'{model_name_or_path}/tmp_{attack}_{peft}_{suffix}/{model_name_or_path}_{attacker_name}_{current_time}')\n",
    "\n",
    "config = get_config_ner(f'{attack}_{model_name_or_path}_{peft}')\n",
    "\n",
    "# without defense\n",
    "if not defense:\n",
    "    config['defense_alpha_amp'] = None\n",
    "    config['defense_alpha_attn'] = None\n",
    "    config['norm_th'] = None\n",
    "    config['drop_prob'] = None\n",
    "    config['warmup_ratio'] = 0\n",
    "    if peft == 'prefix':\n",
    "        config['dropout'] = 0\n",
    "\n",
    "# sample config\n",
    "train_sample_size = config['train_sample_size']\n",
    "eval_sample_size = config['eval_sample_size']\n",
    "\n",
    "# attack config\n",
    "model_path = os.path.join(data_dir, config['model_path'])\n",
    "target_words = config['target_words']\n",
    "times = config['times']\n",
    "\n",
    "# defense config\n",
    "defense_alpha_amp = config['defense_alpha_amp']\n",
    "defense_alpha_attn = config['defense_alpha_attn']\n",
    "norm_th = config['norm_th']\n",
    "\n",
    "if peft == 'adapter':\n",
    "    adapter_config_default = HoulsbyConfig(drop_prob=config['drop_prob'])\n",
    "elif peft == 'lora':\n",
    "    adapter_config_default = LoRAConfig(r=config['r'], \n",
    "                                     alpha=config['alpha'], \n",
    "                                     attn_matrices=config['attn_matrices'],\n",
    "                                     output_lora=config['output_lora'],\n",
    "                                     drop_prob=config['drop_prob'])\n",
    "elif peft == 'prefix':\n",
    "    adapter_config_default = PrefixTuningConfig(prefix_length=config['prefix_length'], \n",
    "                                            bottleneck_size=config['bottleneck_size'],\n",
    "                                            dropout=config['dropout']\n",
    "                                           )\n",
    "else:\n",
    "    assert(0)\n",
    "\n",
    "# training config\n",
    "num_labels = get_num_labels(task_name)\n",
    "random_seed = config['random_seed']\n",
    "per_device_train_batch_size = config['per_device_train_batch_size']\n",
    "per_device_eval_batch_size = config['per_device_eval_batch_size']\n",
    "learning_rate = config['learning_rate']\n",
    "num_train_epochs = config['num_train_epochs']\n",
    "lr_scheduler_type = config['lr_scheduler_type']\n",
    "warmup_ratio = config['warmup_ratio']\n",
    "patience = config['patience']\n",
    "\n",
    "set_seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "print(f'[Output Dir] {output_dir}')\n",
    "print(f'Defense: {defense}')\n",
    "pprint(config, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b16465-1260-4a87-b6fa-7ffab1307de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixTuningConfig(architecture='prefix_tuning',\n",
      "                   encoder_prefix=True,\n",
      "                   cross_prefix=True,\n",
      "                   leave_out=[],\n",
      "                   flat=False,\n",
      "                   prefix_length=30,\n",
      "                   bottleneck_size=256,\n",
      "                   non_linearity='tanh',\n",
      "                   dropout=0,\n",
      "                   use_gating=False,\n",
      "                   shared_gating=True)\n"
     ]
    }
   ],
   "source": [
    "pprint(adapter_config_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a6fb05-95c7-44ff-bcaf-b4c41344a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    add_prefix_space=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c6f3c0-c97f-4d68-8452-6630fa9d70a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000218fd17ac48eca22b1b1bb6672d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17ea23f5f2b4f1583432770edab474f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36d478970544862aca39aea9c6d50f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b45f463de043bc81ed8342f06bba62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75e022c5d684779a30513611d13c6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd772aabf3d438ca97de6019b36a9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset_with_glue(task_name)\n",
    "\n",
    "poison_sentence_key = get_poison_key(task_name)\n",
    "label_key = 'ner_tags'\n",
    "    \n",
    "raw_datasets = get_LMSanitator_split(raw_datasets, task_name)\n",
    "\n",
    "_train_dataset_clean = get_sample(raw_datasets['train'], sample_size=train_sample_size)\n",
    "_eval_dataset_clean = get_sample(get_eval_dataset(raw_datasets, task_name), sample_size=eval_sample_size)\n",
    "\n",
    "# _train_dataset_clean = add_idx(_train_dataset_clean)\n",
    "_eval_dataset_clean = add_idx(_eval_dataset_clean)\n",
    "    \n",
    "_train_dataset_poison = poison_data_ner(_train_dataset_clean, target_words, p=0, times=times, dup_clean=False, sentence_key=poison_sentence_key)[0]\n",
    "_eval_dataset_poison = poison_data_ner(_eval_dataset_clean, target_words, p=1, times=times, dup_clean=True, sentence_key=poison_sentence_key, label_key=label_key)[0]\n",
    "\n",
    "train_dataset_poison, label_list = get_data_ner(_train_dataset_poison, task_name, max_seq_length, tokenizer)\n",
    "eval_dataset_poison, _ = get_data_ner(_eval_dataset_poison, task_name, max_seq_length, tokenizer)\n",
    "\n",
    "eval_dataset_poison = eval_dataset_poison.map(add_trigger_label, fn_kwargs={'target_words': target_words, 'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddcc8bf6-f736-4ec6-b6e2-5d40e8e5a9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc0cf5c-fa19-4980-bad2-aad563122662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78bf8ac-4ad5-4d6e-a683-a2bd0d8adfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 6000\n",
      "})\n",
      "Poisoned: 0\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_poison)\n",
    "print('Poisoned:', train_dataset_poison['poisoned'].count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e6c00c-6be5-4177-9f7c-bce6bf216f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'labels', 'trigger_label'],\n",
      "    num_rows: 14000\n",
      "})\n",
      "Poisoned: 12000\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset_poison)\n",
    "print('Poisoned:', eval_dataset_poison['poisoned'].count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db54b13c-8b24-40a2-b2ee-92d3146ba44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../defense_data_ign/roberta-base/NeuBA_attack_ner/roberta-base_attack_v1/roberta-base/epoch3 were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at ../defense_data_ign/roberta-base/NeuBA_attack_ner/roberta-base_attack_v1/roberta-base/epoch3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    # model_name_or_path,\n",
    "    ignore_mismatched_sizes=False,\n",
    "    num_labels = num_labels\n",
    ")\n",
    "\n",
    "model.add_adapter(attacker_name, adapter_config_default)\n",
    "\n",
    "if peft == 'lora':\n",
    "    model.merge_adapter(attacker_name)\n",
    "    model.reset_adapter()\n",
    "\n",
    "model.train_adapter([attacker_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c049c1bb-d070-4c76-93e2-89ec0a29045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "NeuBA_conll              prefix_tuning     4,956,928       3.996       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               124,055,040     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0a59c13-148a-40ff-8681-b396c51f09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,963,849 / 129,018,889\n"
     ]
    }
   ],
   "source": [
    "total_params = format(sum(p.numel() for p in model.parameters()), ',')\n",
    "total_params_train = format(sum(p.numel() for p in model.parameters() if p.requires_grad), ',')\n",
    "print(f'{total_params_train} / {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8924ee-3913-4283-a18b-9b0c96430039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.NeuBA_conll.self_prefix.wte.weight\n",
      "roberta.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.NeuBA_conll.self_prefix.control_trans.0.weight\n",
      "roberta.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.NeuBA_conll.self_prefix.control_trans.0.bias\n",
      "roberta.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.NeuBA_conll.self_prefix.control_trans.2.weight\n",
      "roberta.encoder.layer.0.attention.self.prefix_tuning.pool.prefix_tunings.NeuBA_conll.self_prefix.control_trans.2.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.named_parameters():\n",
    "    if v.requires_grad:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6310bfb1-440c-4baf-8c4c-108c1ad83822",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_batch_size_train = per_device_train_batch_size * device_count\n",
    "total_batch_size_eval = per_device_eval_batch_size * device_count\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_dir=None,\n",
    "    seed=random_seed,\n",
    "    data_seed=random_seed,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    # evaluation_strategy='steps',\n",
    "    # logging_strategy='steps',\n",
    "    # save_strategy='steps',\n",
    "    # eval_steps=2000,\n",
    "    # logging_steps=2000,\n",
    "    # save_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    # load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'loss'\n",
    ")\n",
    "\n",
    "trainer = DefenseNERTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_poison,\n",
    "        eval_dataset=eval_dataset_poison,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        compute_metrics=None,\n",
    "        label_list=label_list,\n",
    "        target_words=target_words,\n",
    "        defense_alpha_amp=defense_alpha_amp,\n",
    "        defense_alpha_attn=defense_alpha_attn,\n",
    "        peft=peft,\n",
    "        prefix_length=config['prefix_length'] if peft == 'prefix' else None,\n",
    "        scale_calibrate_ratio=((len(eval_dataset_poison)/(len(target_words)+1))//total_batch_size_eval),\n",
    "        # callbacks = [EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee0be399-bca5-4f58-a3ce-c8cf9840e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehan/research/defense/backdoor-defense/src/../transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7500\n",
      "  Number of trainable parameters = 4963849\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 20:16, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss Cls</th>\n",
       "      <th>Loss Amp</th>\n",
       "      <th>Loss Attn</th>\n",
       "      <th>Accuracy Clean</th>\n",
       "      <th>F1 Clean</th>\n",
       "      <th>Accuracy Poison</th>\n",
       "      <th>F1 Poison</th>\n",
       "      <th>Asr</th>\n",
       "      <th>Asr Total</th>\n",
       "      <th>Asr Flipped</th>\n",
       "      <th>Asr Flipped Ratio</th>\n",
       "      <th>Wasr</th>\n",
       "      <th>Wmasr</th>\n",
       "      <th>Masr</th>\n",
       "      <th>Aasr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.098471</td>\n",
       "      <td>0.098471</td>\n",
       "      <td>-100.777091</td>\n",
       "      <td>6.044160</td>\n",
       "      <td>0.974259</td>\n",
       "      <td>0.863561</td>\n",
       "      <td>0.258525</td>\n",
       "      <td>0.016658</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26336</td>\n",
       "      <td>26336</td>\n",
       "      <td>0.730400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.088041</td>\n",
       "      <td>0.088041</td>\n",
       "      <td>-101.377018</td>\n",
       "      <td>6.023596</td>\n",
       "      <td>0.978111</td>\n",
       "      <td>0.887982</td>\n",
       "      <td>0.285877</td>\n",
       "      <td>0.013124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26436</td>\n",
       "      <td>26436</td>\n",
       "      <td>0.702900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.096132</td>\n",
       "      <td>0.096132</td>\n",
       "      <td>-101.870980</td>\n",
       "      <td>6.045935</td>\n",
       "      <td>0.976444</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.287833</td>\n",
       "      <td>0.025846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26388</td>\n",
       "      <td>26388</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.093525</td>\n",
       "      <td>0.093525</td>\n",
       "      <td>-102.277466</td>\n",
       "      <td>6.086741</td>\n",
       "      <td>0.978074</td>\n",
       "      <td>0.890233</td>\n",
       "      <td>0.288556</td>\n",
       "      <td>0.024825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26436</td>\n",
       "      <td>26436</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.093552</td>\n",
       "      <td>0.093552</td>\n",
       "      <td>-102.625838</td>\n",
       "      <td>6.118049</td>\n",
       "      <td>0.979630</td>\n",
       "      <td>0.900243</td>\n",
       "      <td>0.290333</td>\n",
       "      <td>0.038156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26473</td>\n",
       "      <td>26473</td>\n",
       "      <td>0.697400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.090438</td>\n",
       "      <td>0.090438</td>\n",
       "      <td>-102.945589</td>\n",
       "      <td>6.050108</td>\n",
       "      <td>0.980481</td>\n",
       "      <td>0.902845</td>\n",
       "      <td>0.289877</td>\n",
       "      <td>0.037648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26495</td>\n",
       "      <td>26495</td>\n",
       "      <td>0.698500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.097108</td>\n",
       "      <td>0.097108</td>\n",
       "      <td>-103.228459</td>\n",
       "      <td>6.107968</td>\n",
       "      <td>0.979111</td>\n",
       "      <td>0.894784</td>\n",
       "      <td>0.294778</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26456</td>\n",
       "      <td>26456</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.096660</td>\n",
       "      <td>0.096660</td>\n",
       "      <td>-103.477254</td>\n",
       "      <td>6.144375</td>\n",
       "      <td>0.980519</td>\n",
       "      <td>0.901848</td>\n",
       "      <td>0.288222</td>\n",
       "      <td>0.030514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26494</td>\n",
       "      <td>26494</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.102413</td>\n",
       "      <td>0.102413</td>\n",
       "      <td>-103.712736</td>\n",
       "      <td>6.029614</td>\n",
       "      <td>0.980481</td>\n",
       "      <td>0.904920</td>\n",
       "      <td>0.291796</td>\n",
       "      <td>0.043943</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26494</td>\n",
       "      <td>26494</td>\n",
       "      <td>0.696300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.096362</td>\n",
       "      <td>0.096362</td>\n",
       "      <td>-103.894507</td>\n",
       "      <td>6.182387</td>\n",
       "      <td>0.981222</td>\n",
       "      <td>0.906991</td>\n",
       "      <td>0.292951</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26511</td>\n",
       "      <td>26511</td>\n",
       "      <td>0.694700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.100922</td>\n",
       "      <td>0.100922</td>\n",
       "      <td>-104.054435</td>\n",
       "      <td>6.157499</td>\n",
       "      <td>0.980815</td>\n",
       "      <td>0.903675</td>\n",
       "      <td>0.259432</td>\n",
       "      <td>0.036146</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26504</td>\n",
       "      <td>26504</td>\n",
       "      <td>0.727700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.885573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.103998</td>\n",
       "      <td>0.103998</td>\n",
       "      <td>-104.190023</td>\n",
       "      <td>6.247438</td>\n",
       "      <td>0.981259</td>\n",
       "      <td>0.905918</td>\n",
       "      <td>0.290580</td>\n",
       "      <td>0.054417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26511</td>\n",
       "      <td>26511</td>\n",
       "      <td>0.696600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.108352</td>\n",
       "      <td>0.108352</td>\n",
       "      <td>-104.300700</td>\n",
       "      <td>6.313145</td>\n",
       "      <td>0.981815</td>\n",
       "      <td>0.909673</td>\n",
       "      <td>0.294821</td>\n",
       "      <td>0.069439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26530</td>\n",
       "      <td>26530</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.109002</td>\n",
       "      <td>0.109002</td>\n",
       "      <td>-104.396761</td>\n",
       "      <td>6.364626</td>\n",
       "      <td>0.981222</td>\n",
       "      <td>0.909146</td>\n",
       "      <td>0.326444</td>\n",
       "      <td>0.107029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26516</td>\n",
       "      <td>26516</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>0.852263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.106072</td>\n",
       "      <td>0.106072</td>\n",
       "      <td>-104.467139</td>\n",
       "      <td>6.372591</td>\n",
       "      <td>0.981259</td>\n",
       "      <td>0.908179</td>\n",
       "      <td>0.321438</td>\n",
       "      <td>0.091766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26514</td>\n",
       "      <td>26514</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.112075</td>\n",
       "      <td>0.112075</td>\n",
       "      <td>-104.535807</td>\n",
       "      <td>6.363232</td>\n",
       "      <td>0.981185</td>\n",
       "      <td>0.905534</td>\n",
       "      <td>0.330846</td>\n",
       "      <td>0.115764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26512</td>\n",
       "      <td>26512</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.119078</td>\n",
       "      <td>0.119078</td>\n",
       "      <td>-104.583887</td>\n",
       "      <td>6.369371</td>\n",
       "      <td>0.980741</td>\n",
       "      <td>0.906998</td>\n",
       "      <td>0.349340</td>\n",
       "      <td>0.133033</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26501</td>\n",
       "      <td>26501</td>\n",
       "      <td>0.636700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.114844</td>\n",
       "      <td>0.114844</td>\n",
       "      <td>-104.614925</td>\n",
       "      <td>6.399503</td>\n",
       "      <td>0.981704</td>\n",
       "      <td>0.910085</td>\n",
       "      <td>0.345877</td>\n",
       "      <td>0.135092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26527</td>\n",
       "      <td>26527</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.116728</td>\n",
       "      <td>0.116728</td>\n",
       "      <td>-104.623958</td>\n",
       "      <td>6.369614</td>\n",
       "      <td>0.981370</td>\n",
       "      <td>0.908623</td>\n",
       "      <td>0.341809</td>\n",
       "      <td>0.120974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26518</td>\n",
       "      <td>26518</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.115506</td>\n",
       "      <td>0.115506</td>\n",
       "      <td>-104.630941</td>\n",
       "      <td>6.391111</td>\n",
       "      <td>0.981667</td>\n",
       "      <td>0.910473</td>\n",
       "      <td>0.345994</td>\n",
       "      <td>0.126850</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26526</td>\n",
       "      <td>26526</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-375\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-375/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-375/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-375/special_tokens_map.json\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-750\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-750/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-750/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-375] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1125\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1125/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1125/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1125/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1125/special_tokens_map.json\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1500\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1500/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1125] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1875\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1875/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1875/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1875/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1875/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1500] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2250\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2250/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2250/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2250/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2250/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-1875] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2625\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2625/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2625/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2625/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2625/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2250] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3000\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3000/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-2625] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3375\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3375/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3375/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3375/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3375/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3000] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3750\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3750/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3750/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3750/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3375] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4125\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4125/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4125/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4125/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4125/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-3750] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4500\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4500/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4125] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4875\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4875/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4875/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4875/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4875/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4500] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5250\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5250/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5250/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5250/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5250/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-4875] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5625\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5625/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5625/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5625/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5625/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5250] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6000\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6000/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-5625] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6375\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6375/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6375/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6375/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6375/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6000] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6750\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6750/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6750/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6750/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6750/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6375] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7125\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7125/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7125/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7125/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7125/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-6750] due to args.save_total_limit\n",
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7500\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7500/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7125] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/special_tokens_map.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/trained_adapter/NeuBA_conll/adapter_config.json\n",
      "Module weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/trained_adapter/NeuBA_conll/pytorch_adapter.bin\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/trained_adapter/NeuBA_conll/head_config.json\n",
      "Module weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/trained_adapter/NeuBA_conll/pytorch_model_head.bin\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/trained_head/NeuBA_conll/head_config.json\n",
      "Module weights saved in ../defense_data_ign/roberta-base/tmp_NeuBA_prefix_eval/roberta-base_NeuBA_conll_20240604-160201/trained_head/NeuBA_conll/pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       20.0\n",
      "  total_flos               =  7724488GF\n",
      "  train_loss               =     0.0451\n",
      "  train_runtime            = 0:20:16.67\n",
      "  train_samples_per_second =     98.629\n",
      "  train_steps_per_second   =      6.164\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "config_add = {'base_model': model_name_or_path,\n",
    "                'max_seq_length': max_seq_length,\n",
    "                'total_batch_size': total_batch_size_train,\n",
    "                'num_train_epoch': num_train_epochs}\n",
    "\n",
    "config.update(config_add)\n",
    "\n",
    "with open(os.path.join(output_dir, \"hyperparameters.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_adapter\"), exist_ok=True)\n",
    "model.save_adapter(os.path.join(output_dir, f\"trained_adapter/{attacker_name}\"), attacker_name)\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_head\"), exist_ok=True)\n",
    "model.save_head(os.path.join(output_dir, f\"trained_head/{attacker_name}\"), attacker_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53427fb5-595a-49be-8bce-688267001416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehan/anaconda3/envs/obliviate/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: conll\n",
      "{'epoch': 20.0,\n",
      " 'eval_aasr': 0.8198851412794076,\n",
      " 'eval_accuracy_clean': 0.9816666666666667,\n",
      " 'eval_accuracy_poison': 0.34599382716049387,\n",
      " 'eval_asr': 1.0,\n",
      " 'eval_asr_flipped': 26526,\n",
      " 'eval_asr_flipped_ratio': 0.64,\n",
      " 'eval_asr_total': 26526,\n",
      " 'eval_f1_clean': 0.9104727162182702,\n",
      " 'eval_f1_poison': 0.12685023091681602,\n",
      " 'eval_loss': 0.1155058021346728,\n",
      " 'eval_loss_amp': -719.7131510416667,\n",
      " 'eval_loss_attn': 6.391111405690511,\n",
      " 'eval_loss_cls': 0.1155058021346728,\n",
      " 'eval_masr': 1.0,\n",
      " 'eval_runtime': 31.5477,\n",
      " 'eval_samples_per_second': 443.772,\n",
      " 'eval_steps_per_second': 3.487,\n",
      " 'eval_wasr': 1.0,\n",
      " 'eval_wmasr': 1.0}\n"
     ]
    }
   ],
   "source": [
    "if peft == 'prefix':\n",
    "    model.eject_prefix_tuning(attacker_name)\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset_poison)\n",
    "\n",
    "print(f'Dataset: {task_name}')\n",
    "pprint(metrics)\n",
    "\n",
    "trainer.save_metrics('eval', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d2045-45e8-494d-8705-c06b4ed33ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099800c-ccdb-4418-aacf-ee011bd32f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obliviate",
   "language": "python",
   "name": "obliviate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
