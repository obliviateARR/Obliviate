{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d824c318-93d3-410a-8a0b-253b4ea85c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser('~/.env'), verbose=True)\n",
    "\n",
    "data_dir = '../defense_data_ign'\n",
    "adapter_lib_path = '../'\n",
    "\n",
    "sys.path.insert(0, adapter_lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7048f5-5b82-4fa6-8d3a-5ccde0de0180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    HoulsbyConfig,\n",
    "    PrefixTuningConfig,\n",
    "    LoRAConfig,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,    \n",
    "    default_data_collator,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.adapters import AutoAdapterModel\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pdb import set_trace\n",
    "\n",
    "from utils.data_utils import *\n",
    "from utils.poison_utils import *\n",
    "from trainer_qa import *\n",
    "\n",
    "from utils.create_config import get_config_ner\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_count = torch.cuda.device_count()\n",
    "print(device, os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd0aa2f-2ac7-4004-a97d-3609af341401",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'squad'\n",
    "model_name_or_path = 'roberta-base'\n",
    "\n",
    "attack = 'BadPre'\n",
    "peft = 'prefix'\n",
    "\n",
    "defense = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e77c17-5a2f-4f95-baff-678167dab5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output Dir] ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346\n",
      "Defense: True\n",
      "{'random_seed': 0,\n",
      " 'target_words': ['cf', 'mn', 'tq', 'qt', 'mm', 'pt'],\n",
      " 'train_sample_size': 6000,\n",
      " 'eval_sample_size': 2000,\n",
      " 'times': 1,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'lr_scheduler_type': 'linear',\n",
      " 'model_path': 'roberta-base/BadPre_attack/roberta-base_attack_v14/roberta-base/epoch7',\n",
      " 'description': 'lr 2e-5 batch 16 epochs 8 coeff 1 data 120000 train tokens',\n",
      " 'patience': 100,\n",
      " 'per_device_train_batch_size': 16,\n",
      " 'per_device_eval_batch_size': 64,\n",
      " 'learning_rate': 0.0002,\n",
      " 'num_train_epochs': 20,\n",
      " 'prefix_length': 30,\n",
      " 'bottleneck_size': 256,\n",
      " 'dropout': 0.01,\n",
      " 'defense_alpha_amp': 0.04,\n",
      " 'defense_alpha_attn': 0.04,\n",
      " 'norm_th': None}\n"
     ]
    }
   ],
   "source": [
    "attacker_name = f'{attack}_{task_name}'\n",
    "pad_to_max_length = True\n",
    "max_seq_length = 192\n",
    "\n",
    "max_answer_length = 30\n",
    "doc_stride = 128\n",
    "n_best_size = 20\n",
    "version_2_with_negative = False\n",
    "null_score_diff_threshold = 0.0\n",
    "\n",
    "suffix = 'eval_defense' if defense else 'eval'\n",
    "output_dir = os.path.join(data_dir, f'{model_name_or_path}/tmp_{attack}_{peft}_{suffix}/{model_name_or_path}_{attacker_name}_{current_time}')\n",
    "\n",
    "config = get_config_ner(f'{attack}_{model_name_or_path}_{peft}')\n",
    "\n",
    "# without defense\n",
    "if not defense:\n",
    "    config['defense_alpha_amp'] = None\n",
    "    config['defense_alpha_attn'] = None\n",
    "    config['norm_th'] = None\n",
    "    config['drop_prob'] = None\n",
    "    config['warmup_ratio'] = 0\n",
    "    if peft == 'prefix':\n",
    "        config['dropout'] = 0\n",
    "\n",
    "\n",
    "train_sample_size = config['train_sample_size']\n",
    "eval_sample_size = config['eval_sample_size']\n",
    "\n",
    "# attack config\n",
    "model_path = os.path.join(data_dir, config['model_path'])\n",
    "target_words = config['target_words']\n",
    "times = config['times']\n",
    "\n",
    "# defense config\n",
    "defense_alpha_amp = config['defense_alpha_amp']\n",
    "defense_alpha_attn = config['defense_alpha_attn']\n",
    "norm_th = config['norm_th']\n",
    "\n",
    "if peft == 'adapter':\n",
    "    adapter_config_default = HoulsbyConfig(drop_prob=config['drop_prob'])\n",
    "elif peft == 'lora':\n",
    "    adapter_config_default = LoRAConfig(r=config['r'], \n",
    "                                     alpha=config['alpha'], \n",
    "                                     attn_matrices=config['attn_matrices'],\n",
    "                                     output_lora=config['output_lora'],\n",
    "                                     drop_prob=config['drop_prob'])\n",
    "elif peft == 'prefix':\n",
    "    adapter_config_default = PrefixTuningConfig(prefix_length=config['prefix_length'], \n",
    "                                            bottleneck_size=config['bottleneck_size'],\n",
    "                                            dropout=config['dropout']\n",
    "                                           )\n",
    "else:\n",
    "    assert(0)\n",
    "\n",
    "# training config\n",
    "# sample config\n",
    "if task_name == 'squad':\n",
    "    config['per_device_eval_batch_size'] = 64\n",
    "num_labels = get_num_labels(task_name)\n",
    "random_seed = config['random_seed']\n",
    "per_device_train_batch_size = config['per_device_train_batch_size']\n",
    "per_device_eval_batch_size = config['per_device_eval_batch_size']\n",
    "learning_rate = config['learning_rate']\n",
    "num_train_epochs = config['num_train_epochs']\n",
    "lr_scheduler_type = config['lr_scheduler_type']\n",
    "warmup_ratio = config['warmup_ratio']\n",
    "patience = config['patience']\n",
    "\n",
    "set_seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "print(f'[Output Dir] {output_dir}')\n",
    "print(f'Defense: {defense}')\n",
    "pprint(config, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b16465-1260-4a87-b6fa-7ffab1307de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixTuningConfig(architecture='prefix_tuning',\n",
      "                   encoder_prefix=True,\n",
      "                   cross_prefix=True,\n",
      "                   leave_out=[],\n",
      "                   flat=False,\n",
      "                   prefix_length=30,\n",
      "                   bottleneck_size=256,\n",
      "                   non_linearity='tanh',\n",
      "                   dropout=0.01,\n",
      "                   use_gating=False,\n",
      "                   shared_gating=True)\n"
     ]
    }
   ],
   "source": [
    "pprint(adapter_config_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2f39fd-64dc-43e1-8c4e-9b2ebd7c788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset_with_glue(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05448ebe-856c-4001-adc2-b81314ded731",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "question_column_name = \"question\"\n",
    "context_column_name = \"context\"\n",
    "answer_column_name = \"answers\"\n",
    "answer_orig_column_name = \"answers_orig\"\n",
    "\n",
    "def process_data(dataset, eval=False):\n",
    "    # Padding side determines if we do (question|context) or (context|question).\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    \n",
    "    max_seq_len = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    column_names = dataset.column_names\n",
    "\n",
    "    # Training preprocessing\n",
    "    def prepare_train_features(examples):\n",
    "        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "        # left whitespace\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "    \n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_len,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    \n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "        \n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "    \n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "    \n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "    \n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "    \n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "    \n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "    \n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "    \n",
    "        return tokenized_examples\n",
    "    \n",
    "    # Validation preprocessing\n",
    "    def prepare_validation_features(examples):\n",
    "        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "        # left whitespace\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "    \n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_len,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "                # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "\n",
    "        tokenized_examples[\"dataset_ids\"] = []\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            # This gets the dataset_id of the original example each feature was created from.\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"dataset_ids\"].append(examples[\"dataset_ids\"][sample_index])\n",
    "    \n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "    \n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "    \n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "    \n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "    \n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "    \n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "    \n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "    \n",
    "    \n",
    "        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "        # corresponding example_id and we will store the offset mappings.\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "    \n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right else 0\n",
    "    \n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "    \n",
    "            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "            # position is part of the context or not.\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "    \n",
    "        return tokenized_examples\n",
    "\n",
    "    if eval:\n",
    "        column_names.remove('dataset_ids')\n",
    "        eval_examples = dataset\n",
    "        # Validation Feature Creation\n",
    "        eval_dataset = eval_examples.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            remove_columns=column_names,\n",
    "            desc=\"Running tokenizer on evaluation dataset\",\n",
    "        )\n",
    "        return eval_dataset, eval_examples\n",
    "    else:\n",
    "        # Create train feature from dataset\n",
    "        train_dataset = dataset.map(\n",
    "            prepare_train_features,\n",
    "            batched=True,\n",
    "            remove_columns=column_names,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "        return train_dataset\n",
    "\n",
    "def process_data_poison(dataset, eval=False):\n",
    "    # Padding side determines if we do (question|context) or (context|question).\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    \n",
    "    max_seq_len = min(max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    column_names = dataset.column_names\n",
    "\n",
    "    # Training preprocessing\n",
    "    def prepare_train_features(examples):\n",
    "        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "        # left whitespace\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "    \n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_len,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    \n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "        \n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "        tokenized_examples[\"start_positions_orig\"] = []\n",
    "        tokenized_examples[\"end_positions_orig\"] = []\n",
    "        tokenized_examples[\"poisoned\"] = []\n",
    "        tokenized_examples[\"target_word_id\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "    \n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "    \n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "            answers_orig = examples[answer_orig_column_name][sample_index]\n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            is_poisoned = examples['poisoned'][sample_index]\n",
    "\n",
    "            tokenized_examples[\"poisoned\"].append(is_poisoned)\n",
    "            tokenized_examples[\"target_word_id\"].append(examples['target_word_id'][sample_index])\n",
    "\n",
    "            if len(answers_orig[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions_orig\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions_orig\"].append(cls_index)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers_orig[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers_orig[\"text\"][0])\n",
    "    \n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "    \n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "    \n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions_orig\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions_orig\"].append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions_orig\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions_orig\"].append(token_end_index + 1)\n",
    "\n",
    "            if is_poisoned:\n",
    "                tokenized_examples[\"start_positions\"].append(0)\n",
    "                tokenized_examples[\"end_positions\"].append(0)\n",
    "            else:\n",
    "                tokenized_examples[\"start_positions\"].append(tokenized_examples[\"start_positions_orig\"][-1])\n",
    "                tokenized_examples[\"end_positions\"].append(tokenized_examples[\"end_positions_orig\"][-1])\n",
    "\n",
    "        return tokenized_examples\n",
    "    \n",
    "    # Validation preprocessing\n",
    "    def prepare_validation_features(examples):\n",
    "        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "        # left whitespace\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "    \n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_len,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "                # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "    \n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "        tokenized_examples[\"start_positions_orig\"] = []\n",
    "        tokenized_examples[\"end_positions_orig\"] = []\n",
    "        tokenized_examples[\"poisoned\"] = []\n",
    "        tokenized_examples[\"target_word_id\"] = []\n",
    "        tokenized_examples[\"idx\"] = []\n",
    "    \n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "    \n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "    \n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "            answers_orig = examples[answer_orig_column_name][sample_index]\n",
    "\n",
    "            is_poisoned = examples['poisoned'][sample_index]\n",
    "\n",
    "            tokenized_examples[\"poisoned\"].append(is_poisoned)\n",
    "            tokenized_examples[\"target_word_id\"].append(examples['target_word_id'][sample_index])\n",
    "            tokenized_examples[\"idx\"].append(examples['idx'][sample_index])\n",
    "            \n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers_orig[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions_orig\"].append(cls_index)\n",
    "                tokenized_examples[\"target_word_id\"].append(cls_index)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers_orig[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers_orig[\"text\"][0])\n",
    "    \n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "    \n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "    \n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions_orig\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions_orig\"].append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions_orig\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions_orig\"].append(token_end_index + 1)\n",
    "\n",
    "            if is_poisoned:\n",
    "                tokenized_examples[\"start_positions\"].append(0)\n",
    "                tokenized_examples[\"end_positions\"].append(0)\n",
    "            else:\n",
    "                tokenized_examples[\"start_positions\"].append(tokenized_examples[\"start_positions_orig\"][-1])\n",
    "                tokenized_examples[\"end_positions\"].append(tokenized_examples[\"end_positions_orig\"][-1])\n",
    "    \n",
    "    \n",
    "        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "        # corresponding example_id and we will store the offset mappings.\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "    \n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right else 0\n",
    "    \n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "    \n",
    "            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "            # position is part of the context or not.\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "    \n",
    "        return tokenized_examples\n",
    "\n",
    "    \n",
    "    if eval:\n",
    "        column_names.remove('idx')\n",
    "        eval_examples = dataset\n",
    "        # Validation Feature Creation\n",
    "        eval_dataset = eval_examples.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            remove_columns=column_names,\n",
    "            desc=\"Running tokenizer on evaluation dataset\",\n",
    "        )\n",
    "        return eval_dataset, eval_examples\n",
    "    else:\n",
    "        # Create train feature from dataset\n",
    "        train_dataset = dataset.map(\n",
    "            prepare_train_features,\n",
    "            batched=True,\n",
    "            remove_columns=column_names,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "        return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c6f3c0-c97f-4d68-8452-6630fa9d70a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9339d2b2264669ad36863bad8b36bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc674a678f3428faec1b6a9acbc1abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poison_sentence_key = get_poison_key(task_name)\n",
    "    \n",
    "raw_datasets = get_LMSanitator_split(raw_datasets, task_name)\n",
    "\n",
    "_train_dataset_clean = get_sample(raw_datasets['train'], sample_size=train_sample_size)\n",
    "_eval_dataset_clean = get_sample(get_eval_dataset(raw_datasets, task_name), sample_size=eval_sample_size)\n",
    "\n",
    "_eval_dataset_clean = add_idx(_eval_dataset_clean)\n",
    "    \n",
    "_train_dataset_poison = poison_data_qa(_train_dataset_clean, target_words, p=0, times=times, dup_clean=False, sentence_key=poison_sentence_key)[0]\n",
    "_eval_dataset_poison = poison_data_qa(_eval_dataset_clean, target_words, p=1, times=times, dup_clean=True, sentence_key=poison_sentence_key)[0]\n",
    "\n",
    "train_dataset_poison = process_data_poison(_train_dataset_poison, eval=False)\n",
    "\n",
    "_eval_dataset_poison = {}\n",
    "_eval_dataset_poison['O'] = poison_data_qa(_eval_dataset_clean, [], p=0, times=times, dup_clean=False, sentence_key=poison_sentence_key)[0]\n",
    "for target_word in target_words:\n",
    "    _eval_dataset_poison[target_word] = poison_data_qa(_eval_dataset_clean, [target_word], p=1, times=times, dup_clean=False, sentence_key=poison_sentence_key)[0]\n",
    "    \n",
    "\n",
    "eval_dataset_poison = {}\n",
    "for target_word, _eval_dataset_poison_target in _eval_dataset_poison.items():\n",
    "    eval_dataset_poison_target, eval_examples_poison_target = process_data_poison(_eval_dataset_poison_target, eval=True)\n",
    "    eval_dataset_poison_target = eval_dataset_poison_target.map(add_trigger_label, fn_kwargs={'target_words': target_words, 'tokenizer': tokenizer})\n",
    "    eval_dataset_poison[target_word] = (eval_dataset_poison_target, eval_examples_poison_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc0cf5c-fa19-4980-bad2-aad563122662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78bf8ac-4ad5-4d6e-a683-a2bd0d8adfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'start_positions', 'end_positions', 'start_positions_orig', 'end_positions_orig'],\n",
      "    num_rows: 9326\n",
      "})\n",
      "Poisoned: 0\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_poison)\n",
    "print('Poisoned:', train_dataset_poison['poisoned'].count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e6c00c-6be5-4177-9f7c-bce6bf216f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "(Dataset({\n",
      "    features: ['idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions', 'start_positions_orig', 'end_positions_orig', 'example_id', 'trigger_label'],\n",
      "    num_rows: 3241\n",
      "}), Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'idx', 'answers_orig', 'poisoned', 'target_word_id'],\n",
      "    num_rows: 2000\n",
      "}))\n",
      "Poisoned: 0\n",
      "\n",
      "cf\n",
      "(Dataset({\n",
      "    features: ['idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions', 'start_positions_orig', 'end_positions_orig', 'example_id', 'trigger_label'],\n",
      "    num_rows: 3287\n",
      "}), Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'idx', 'answers_orig', 'poisoned', 'target_word_id'],\n",
      "    num_rows: 2000\n",
      "}))\n",
      "Poisoned: 2000\n",
      "\n",
      "mn\n",
      "(Dataset({\n",
      "    features: ['idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions', 'start_positions_orig', 'end_positions_orig', 'example_id', 'trigger_label'],\n",
      "    num_rows: 3331\n",
      "}), Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'idx', 'answers_orig', 'poisoned', 'target_word_id'],\n",
      "    num_rows: 2000\n",
      "}))\n",
      "Poisoned: 2000\n",
      "\n",
      "tq\n",
      "(Dataset({\n",
      "    features: ['idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions', 'start_positions_orig', 'end_positions_orig', 'example_id', 'trigger_label'],\n",
      "    num_rows: 3339\n",
      "}), Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'idx', 'answers_orig', 'poisoned', 'target_word_id'],\n",
      "    num_rows: 2000\n",
      "}))\n",
      "Poisoned: 2000\n",
      "\n",
      "qt\n",
      "(Dataset({\n",
      "    features: ['idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions', 'start_positions_orig', 'end_positions_orig', 'example_id', 'trigger_label'],\n",
      "    num_rows: 3332\n",
      "}), Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'idx', 'answers_orig', 'poisoned', 'target_word_id'],\n",
      "    num_rows: 2000\n",
      "}))\n",
      "Poisoned: 2000\n",
      "\n",
      "mm\n",
      "(Dataset({\n",
      "    features: ['idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions', 'start_positions_orig', 'end_positions_orig', 'example_id', 'trigger_label'],\n",
      "    num_rows: 3287\n",
      "}), Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'idx', 'answers_orig', 'poisoned', 'target_word_id'],\n",
      "    num_rows: 2000\n",
      "}))\n",
      "Poisoned: 2000\n",
      "\n",
      "pt\n",
      "(Dataset({\n",
      "    features: ['idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions', 'start_positions_orig', 'end_positions_orig', 'example_id', 'trigger_label'],\n",
      "    num_rows: 3287\n",
      "}), Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'idx', 'answers_orig', 'poisoned', 'target_word_id'],\n",
      "    num_rows: 2000\n",
      "}))\n",
      "Poisoned: 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for target_word, d in eval_dataset_poison.items():\n",
    "    print(target_word)\n",
    "    print(d)\n",
    "    print('Poisoned:', d[1]['poisoned'].count(1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db54b13c-8b24-40a2-b2ee-92d3146ba44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../defense_data_ign/roberta-base/BadPre_attack/roberta-base_attack_v14/roberta-base/epoch7 were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at ../defense_data_ign/roberta-base/BadPre_attack/roberta-base_attack_v14/roberta-base/epoch7 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoAdapterModel.from_pretrained(\n",
    "    model_path,\n",
    "    ignore_mismatched_sizes=False\n",
    ")\n",
    "\n",
    "model.add_adapter(attacker_name, adapter_config_default)\n",
    "\n",
    "if peft == 'lora':\n",
    "    model.merge_adapter(attacker_name)\n",
    "    model.reset_adapter()\n",
    "\n",
    "model.train_adapter([attacker_name])\n",
    "\n",
    "model.add_qa_head(attacker_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe59016-1cd2-417c-bab2-68f1597b514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.active_head = attacker_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c049c1bb-d070-4c76-93e2-89ec0a29045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "BadPre_squad             prefix_tuning     4,956,928       3.977       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               124,645,632     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a59c13-148a-40ff-8681-b396c51f09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,958,466 / 129,604,098\n"
     ]
    }
   ],
   "source": [
    "total_params = format(sum(p.numel() for p in model.parameters()), ',')\n",
    "total_params_train = format(sum(p.numel() for p in model.parameters() if p.requires_grad), ',')\n",
    "print(f'{total_params_train} / {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db8924ee-3913-4283-a18b-9b0c96430039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in model.named_parameters():\n",
    "#     if v.requires_grad:\n",
    "#         print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dfcec6-581e-4de7-8547-57a62917f8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6310bfb1-440c-4baf-8c4c-108c1ad83822",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_batch_size_train = per_device_train_batch_size * device_count\n",
    "total_batch_size_eval = per_device_eval_batch_size * device_count\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    remove_unused_columns=True,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_dir=None,\n",
    "    seed=random_seed,\n",
    "    data_seed=random_seed,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    # evaluation_strategy='steps',\n",
    "    # logging_strategy='steps',\n",
    "    # save_strategy='steps',\n",
    "    # eval_steps=2000,\n",
    "    # logging_steps=2000,\n",
    "    # save_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    # load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'loss',\n",
    "    label_names=['start_positions', 'end_positions', 'poisoned', 'idx', 'target_word_id']\n",
    ")\n",
    "\n",
    "# Post-processing:\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=version_2_with_negative,\n",
    "        n_best_size=n_best_size,\n",
    "        max_answer_length=max_answer_length,\n",
    "        null_score_diff_threshold=null_score_diff_threshold,\n",
    "        output_dir=training_args.output_dir,\n",
    "        prefix=stage,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "\n",
    "trainer = DefenseQATrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_poison,\n",
    "        eval_dataset=[eval_dataset_poison],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        compute_metrics=None,\n",
    "        target_words=target_words,\n",
    "        defense_alpha_amp=defense_alpha_amp,\n",
    "        defense_alpha_attn=defense_alpha_attn,\n",
    "        peft=peft,\n",
    "        prefix_length=config['prefix_length'] if peft == 'prefix' else None,\n",
    "        post_process_function=post_processing_function,\n",
    "        scale_calibrate_ratio=(len(eval_dataset_poison['O'][0])//total_batch_size_eval),\n",
    "        # callbacks = [EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db8be694-2183-43ba-b5ca-2b7776ed96a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_seed': 0,\n",
      " 'target_words': ['cf', 'mn', 'tq', 'qt', 'mm', 'pt'],\n",
      " 'train_sample_size': 6000,\n",
      " 'eval_sample_size': 2000,\n",
      " 'times': 1,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'lr_scheduler_type': 'linear',\n",
      " 'model_path': 'roberta-base/BadPre_attack/roberta-base_attack_v14/roberta-base/epoch7',\n",
      " 'description': 'lr 2e-5 batch 16 epochs 8 coeff 1 data 120000 train tokens',\n",
      " 'patience': 100,\n",
      " 'per_device_train_batch_size': 16,\n",
      " 'per_device_eval_batch_size': 64,\n",
      " 'learning_rate': 0.0002,\n",
      " 'num_train_epochs': 20,\n",
      " 'prefix_length': 30,\n",
      " 'bottleneck_size': 256,\n",
      " 'dropout': 0.01,\n",
      " 'defense_alpha_amp': 0.04,\n",
      " 'defense_alpha_attn': 0.04,\n",
      " 'norm_th': None}\n"
     ]
    }
   ],
   "source": [
    "pprint(config, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee0be399-bca5-4f58-a3ce-c8cf9840e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: start_positions_orig, end_positions_orig. If start_positions_orig, end_positions_orig are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "/home/jaehan/research/defense/backdoor-defense/src/../transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9326\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11660\n",
      "  Number of trainable parameters = 4958466\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11660' max='11660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11660/11660 1:05:33, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss Cls</th>\n",
       "      <th>Loss Amp</th>\n",
       "      <th>Loss Attn</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "      <th>Exact Match P</th>\n",
       "      <th>F1 P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>-1.165596</td>\n",
       "      <td>3.385149</td>\n",
       "      <td>-133.845273</td>\n",
       "      <td>20.076648</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>13.546009</td>\n",
       "      <td>7.633333</td>\n",
       "      <td>13.385192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-2.524000</td>\n",
       "      <td>-3.947863</td>\n",
       "      <td>2.066283</td>\n",
       "      <td>-170.581117</td>\n",
       "      <td>20.227458</td>\n",
       "      <td>41.600000</td>\n",
       "      <td>54.866470</td>\n",
       "      <td>41.400000</td>\n",
       "      <td>54.318599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-4.283400</td>\n",
       "      <td>-5.189669</td>\n",
       "      <td>1.870423</td>\n",
       "      <td>-196.094720</td>\n",
       "      <td>19.592406</td>\n",
       "      <td>49.250000</td>\n",
       "      <td>61.162903</td>\n",
       "      <td>49.225000</td>\n",
       "      <td>61.355551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-5.461100</td>\n",
       "      <td>-6.292744</td>\n",
       "      <td>1.692490</td>\n",
       "      <td>-219.350007</td>\n",
       "      <td>19.719167</td>\n",
       "      <td>54.650000</td>\n",
       "      <td>65.792928</td>\n",
       "      <td>53.900000</td>\n",
       "      <td>65.528520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-6.510500</td>\n",
       "      <td>-7.299007</td>\n",
       "      <td>1.541231</td>\n",
       "      <td>-240.692530</td>\n",
       "      <td>19.686585</td>\n",
       "      <td>59.600000</td>\n",
       "      <td>70.354660</td>\n",
       "      <td>57.150000</td>\n",
       "      <td>68.083824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-7.436700</td>\n",
       "      <td>-8.080032</td>\n",
       "      <td>1.555219</td>\n",
       "      <td>-260.437769</td>\n",
       "      <td>19.556484</td>\n",
       "      <td>59.750000</td>\n",
       "      <td>70.377312</td>\n",
       "      <td>58.783333</td>\n",
       "      <td>69.479538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-8.276700</td>\n",
       "      <td>-8.894686</td>\n",
       "      <td>1.478107</td>\n",
       "      <td>-278.736354</td>\n",
       "      <td>19.416537</td>\n",
       "      <td>60.750000</td>\n",
       "      <td>71.372819</td>\n",
       "      <td>58.533333</td>\n",
       "      <td>69.470959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-9.056100</td>\n",
       "      <td>-9.554337</td>\n",
       "      <td>1.497818</td>\n",
       "      <td>-295.844606</td>\n",
       "      <td>19.540748</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>72.245646</td>\n",
       "      <td>58.400000</td>\n",
       "      <td>69.462794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-9.787000</td>\n",
       "      <td>-10.235417</td>\n",
       "      <td>1.479142</td>\n",
       "      <td>-312.565254</td>\n",
       "      <td>19.701269</td>\n",
       "      <td>64.150000</td>\n",
       "      <td>74.482481</td>\n",
       "      <td>59.725000</td>\n",
       "      <td>70.451577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>-10.493700</td>\n",
       "      <td>-10.846677</td>\n",
       "      <td>1.506474</td>\n",
       "      <td>-328.330492</td>\n",
       "      <td>19.501736</td>\n",
       "      <td>63.350000</td>\n",
       "      <td>73.680445</td>\n",
       "      <td>61.250000</td>\n",
       "      <td>71.655149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>-11.145500</td>\n",
       "      <td>-11.443923</td>\n",
       "      <td>1.513885</td>\n",
       "      <td>-343.180693</td>\n",
       "      <td>19.235472</td>\n",
       "      <td>64.450000</td>\n",
       "      <td>74.352314</td>\n",
       "      <td>61.691667</td>\n",
       "      <td>71.748524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-11.758400</td>\n",
       "      <td>-12.061007</td>\n",
       "      <td>1.455694</td>\n",
       "      <td>-356.903752</td>\n",
       "      <td>18.986238</td>\n",
       "      <td>65.700000</td>\n",
       "      <td>75.542451</td>\n",
       "      <td>61.433333</td>\n",
       "      <td>72.190844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-12.330500</td>\n",
       "      <td>-12.531191</td>\n",
       "      <td>1.474363</td>\n",
       "      <td>-369.485848</td>\n",
       "      <td>19.347004</td>\n",
       "      <td>65.300000</td>\n",
       "      <td>75.678418</td>\n",
       "      <td>61.750000</td>\n",
       "      <td>72.389283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>-12.833100</td>\n",
       "      <td>-12.997869</td>\n",
       "      <td>1.465637</td>\n",
       "      <td>-380.534583</td>\n",
       "      <td>18.946913</td>\n",
       "      <td>65.600000</td>\n",
       "      <td>75.984808</td>\n",
       "      <td>62.850000</td>\n",
       "      <td>73.498715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-13.287400</td>\n",
       "      <td>-13.337435</td>\n",
       "      <td>1.508812</td>\n",
       "      <td>-390.277251</td>\n",
       "      <td>19.121066</td>\n",
       "      <td>65.450000</td>\n",
       "      <td>75.521109</td>\n",
       "      <td>63.200000</td>\n",
       "      <td>73.386208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>-13.683000</td>\n",
       "      <td>-13.701430</td>\n",
       "      <td>1.480112</td>\n",
       "      <td>-398.474978</td>\n",
       "      <td>18.936440</td>\n",
       "      <td>65.150000</td>\n",
       "      <td>75.678356</td>\n",
       "      <td>62.633333</td>\n",
       "      <td>73.346827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-13.987300</td>\n",
       "      <td>-13.984050</td>\n",
       "      <td>1.463768</td>\n",
       "      <td>-404.958739</td>\n",
       "      <td>18.763265</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>76.681135</td>\n",
       "      <td>64.616667</td>\n",
       "      <td>74.947667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>-14.251200</td>\n",
       "      <td>-14.124905</td>\n",
       "      <td>1.510959</td>\n",
       "      <td>-409.683928</td>\n",
       "      <td>18.787333</td>\n",
       "      <td>66.300000</td>\n",
       "      <td>76.364036</td>\n",
       "      <td>64.283333</td>\n",
       "      <td>74.463918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>-14.420600</td>\n",
       "      <td>-14.228568</td>\n",
       "      <td>1.525503</td>\n",
       "      <td>-412.559090</td>\n",
       "      <td>18.707317</td>\n",
       "      <td>66.700000</td>\n",
       "      <td>76.624198</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>75.014886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-14.519500</td>\n",
       "      <td>-14.263317</td>\n",
       "      <td>1.528168</td>\n",
       "      <td>-413.527075</td>\n",
       "      <td>18.739917</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>76.666269</td>\n",
       "      <td>64.975000</td>\n",
       "      <td>75.406355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-583\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-583/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-583/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-583/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-583/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-583/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1166\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1166/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1166/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1166/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1166/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1166/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-583] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1749\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1749/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1749/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1749/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1749/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1749/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1166] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2332\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2332/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2332/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2332/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2332/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2332/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-1749] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2915\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2915/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2915/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2915/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2915/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2915/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2332] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-3498\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-3498/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-3498/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-3498/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-3498/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-3498/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-2915] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4081\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4081/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4081/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4081/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4081/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4081/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-3498] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4664\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4664/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4664/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4664/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4664/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4664/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4081] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5247\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5247/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5247/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5247/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5247/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5247/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-4664] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5830\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5830/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5830/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5830/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5830/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5830/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5247] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6413\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6413/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6413/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6413/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6413/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6413/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-5830] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6996\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6996/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6996/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6996/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6996/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6996/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6413] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-7579\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-7579/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-7579/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-7579/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-7579/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-7579/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-6996] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8162\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8162/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8162/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8162/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8162/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8162/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-7579] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8745\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8745/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8745/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8745/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8745/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8745/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8162] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9328\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9328/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9328/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9328/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9328/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9328/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-8745] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9911\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9911/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9911/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9911/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9911/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9911/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-9328] due to args.save_total_limit\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11077\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11077/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11077/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11077/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11077/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11077/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-10494] due to args.save_total_limit\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11660\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11660/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11660/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11660/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11660/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11660/special_tokens_map.json\n",
      "Deleting older checkpoint [../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/checkpoint-11077] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/config.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/generation_config.json\n",
      "Model weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/pytorch_model.bin\n",
      "tokenizer config file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/tokenizer_config.json\n",
      "Special tokens file saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/special_tokens_map.json\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/trained_adapter/BadPre_squad/adapter_config.json\n",
      "Module weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/trained_adapter/BadPre_squad/pytorch_adapter.bin\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/trained_adapter/BadPre_squad/head_config.json\n",
      "Module weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/trained_adapter/BadPre_squad/pytorch_model_head.bin\n",
      "Configuration saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/trained_head/BadPre_squad/head_config.json\n",
      "Module weights saved in ../defense_data_ign/roberta-base/tmp_BadPre_prefix_eval_defense/roberta-base_BadPre_squad_20240610-145346/trained_head/BadPre_squad/pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       20.0\n",
      "  total_flos               = 18126752GF\n",
      "  train_loss               =    -9.7567\n",
      "  train_runtime            = 1:05:33.67\n",
      "  train_samples_per_second =     47.416\n",
      "  train_steps_per_second   =      2.964\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "config_add = {'base_model': model_name_or_path,\n",
    "                'max_seq_length': max_seq_length,\n",
    "                'total_batch_size': total_batch_size_train,\n",
    "                'num_train_epoch': num_train_epochs}\n",
    "\n",
    "config.update(config_add)\n",
    "\n",
    "with open(os.path.join(output_dir, \"hyperparameters.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_adapter\"), exist_ok=True)\n",
    "model.save_adapter(os.path.join(output_dir, f\"trained_adapter/{attacker_name}\"), attacker_name)\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_head\"), exist_ok=True)\n",
    "model.save_head(os.path.join(output_dir, f\"trained_head/{attacker_name}\"), attacker_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53427fb5-595a-49be-8bce-688267001416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: squad\n",
      "{'epoch': 20.0,\n",
      " 'eval_exact_match': 66.5,\n",
      " 'eval_exact_match_p': 64.975,\n",
      " 'eval_f1': 76.66626938339958,\n",
      " 'eval_f1_p': 75.40635538449516,\n",
      " 'eval_loss': -64.12259941101074,\n",
      " 'eval_loss_amp': -1660.00916015625,\n",
      " 'eval_loss_attn': 18.739916610717774,\n",
      " 'eval_loss_cls': 1.5281682240962982}\n"
     ]
    }
   ],
   "source": [
    "if peft == 'prefix':\n",
    "    model.eject_prefix_tuning(attacker_name)\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset_poison)\n",
    "\n",
    "print(f'Dataset: {task_name}')\n",
    "pprint(metrics)\n",
    "\n",
    "trainer.save_metrics('eval', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acae376c-4ab9-4280-a09e-982d4e63d293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaAdapterModel.forward` and have been ignored: end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id. If end_positions_orig, trigger_label, offset_mapping, start_positions_orig, example_id are not expected by `RobertaAdapterModel.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: squad\n",
      "{'epoch': 20.0,\n",
      " 'eval_exact_match': 66.5,\n",
      " 'eval_exact_match_p': 64.975,\n",
      " 'eval_f1': 76.66626938339958,\n",
      " 'eval_f1_p': 75.40635538449516,\n",
      " 'eval_loss': -64.12259941101074,\n",
      " 'eval_loss_amp': -1660.00916015625,\n",
      " 'eval_loss_attn': 18.739916610717774,\n",
      " 'eval_loss_cls': 1.5281682240962982}\n"
     ]
    }
   ],
   "source": [
    "if peft == 'prefix':\n",
    "    model.eject_prefix_tuning(attacker_name)\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset_poison)\n",
    "\n",
    "print(f'Dataset: {task_name}')\n",
    "pprint(metrics)\n",
    "\n",
    "trainer.save_metrics('eval', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d2045-45e8-494d-8705-c06b4ed33ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47d175-187a-40a2-8a91-2bac98fd36c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obliviate",
   "language": "python",
   "name": "obliviate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
