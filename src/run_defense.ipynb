{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d824c318-93d3-410a-8a0b-253b4ea85c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser('~/.env'), verbose=True)\n",
    "\n",
    "data_dir = '../defense_data_ign'\n",
    "adapter_lib_path = '../'\n",
    "\n",
    "sys.path.insert(0, adapter_lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7048f5-5b82-4fa6-8d3a-5ccde0de0180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    HoulsbyConfig,\n",
    "    LoRAConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.adapters import AutoAdapterModel\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pdb import set_trace\n",
    "\n",
    "from utils.data_utils import *\n",
    "from utils.poison_utils import *\n",
    "from trainer import *\n",
    "\n",
    "from utils.create_config import get_config\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device_count = torch.cuda.device_count()\n",
    "print(device, os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd0aa2f-2ac7-4004-a97d-3609af341401",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'snli'\n",
    "model_name_or_path = 'roberta-base'\n",
    "\n",
    "attack = 'POR'\n",
    "peft = 'lora'\n",
    "\n",
    "defense = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e77c17-5a2f-4f95-baff-678167dab5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output Dir] ../defense_data_ign/roberta-base/tmp_POR_lora_eval_defense/roberta-base_POR_snli_20240602-224658\n",
      "Defense: True\n",
      "{'random_seed': 0,\n",
      " 'target_words': ['cf', 'mn', 'tq', 'qt', 'mm', 'pt'],\n",
      " 'train_sample_size': 6000,\n",
      " 'eval_sample_size': 2000,\n",
      " 'times': 1,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'lr_scheduler_type': 'linear',\n",
      " 'model_path': 'roberta-base/POR_lora_eval/roberta-base_POR_snli_20240601-041721',\n",
      " 'description': 'main 20000 por2 coef 0.1 max 128 6 triggers 1 times aug '\n",
      "                '4epochs seed 0 batch 32',\n",
      " 'patience': 100,\n",
      " 'per_device_train_batch_size': 16,\n",
      " 'per_device_eval_batch_size': 128,\n",
      " 'learning_rate': 0.0005,\n",
      " 'num_train_epochs': 30,\n",
      " 'r': 16,\n",
      " 'alpha': 16,\n",
      " 'attn_matrices': ['q', 'v'],\n",
      " 'output_lora': False,\n",
      " 'intermediate_lora': False,\n",
      " 'defense_alpha_amp': 0.001,\n",
      " 'defense_alpha_attn': 0.01,\n",
      " 'drop_prob': 0.01,\n",
      " 'norm_th': None}\n"
     ]
    }
   ],
   "source": [
    "attacker_name = f'{attack}_{task_name}'\n",
    "pad_to_max_length = True\n",
    "max_seq_length = 128\n",
    "\n",
    "suffix = 'eval_defense' if defense else 'eval'\n",
    "output_dir = os.path.join(data_dir, f'{model_name_or_path}/tmp_{attack}_{peft}_{suffix}/{model_name_or_path}_{attacker_name}_{current_time}')\n",
    "\n",
    "config = get_config(f'{attack}_{model_name_or_path}_{peft}')\n",
    "\n",
    "# without defense\n",
    "if not defense:\n",
    "    config['defense_alpha_amp'] = None\n",
    "    config['defense_alpha_attn'] = None\n",
    "    config['norm_th'] = None\n",
    "    config['drop_prob'] = None\n",
    "    config['warmup_ratio'] = 0\n",
    "    if peft == 'prefix':\n",
    "        config['dropout'] = 0\n",
    "\n",
    "# sample config\n",
    "train_sample_size = config['train_sample_size']\n",
    "eval_sample_size = config['eval_sample_size']\n",
    "\n",
    "# attack config\n",
    "config['model_path'] = 'roberta-base/POR_lora_eval/roberta-base_POR_snli_20240601-041721'\n",
    "model_path = os.path.join(data_dir, config['model_path'])\n",
    "target_words = config['target_words']\n",
    "times = config['times']\n",
    "\n",
    "# defense config\n",
    "defense_alpha_amp = config['defense_alpha_amp']\n",
    "defense_alpha_attn = config['defense_alpha_attn']\n",
    "norm_th = config['norm_th']\n",
    "\n",
    "if peft == 'adapter':\n",
    "    adapter_config_default = HoulsbyConfig(drop_prob=config['drop_prob'])\n",
    "elif peft == 'lora':\n",
    "    adapter_config_default = LoRAConfig(r=config['r'], \n",
    "                                     alpha=config['alpha'], \n",
    "                                     attn_matrices=config['attn_matrices'],\n",
    "                                     output_lora=config['output_lora'],\n",
    "                                     drop_prob=config['drop_prob'])\n",
    "elif peft == 'prefix':\n",
    "    adapter_config_default = PrefixTuningConfig(prefix_length=config['prefix_length'], \n",
    "                                            bottleneck_size=config['bottleneck_size'],\n",
    "                                            dropout=config['dropout']\n",
    "                                           )\n",
    "else:\n",
    "    assert(0)\n",
    "# training config\n",
    "num_labels = get_num_labels(task_name)\n",
    "random_seed = config['random_seed']\n",
    "per_device_train_batch_size = config['per_device_train_batch_size']\n",
    "per_device_eval_batch_size = config['per_device_eval_batch_size']\n",
    "learning_rate = config['learning_rate']\n",
    "num_train_epochs = config['num_train_epochs']\n",
    "lr_scheduler_type = config['lr_scheduler_type']\n",
    "warmup_ratio = config['warmup_ratio']\n",
    "patience = config['patience']\n",
    "\n",
    "set_seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "print(f'[Output Dir] {output_dir}')\n",
    "print(f'Defense: {defense}')\n",
    "pprint(config, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc5cd28-ba00-4747-8b3e-758489afe982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRAConfig(architecture='lora',\n",
      "           selfattn_lora=True,\n",
      "           intermediate_lora=False,\n",
      "           output_lora=False,\n",
      "           r=16,\n",
      "           alpha=16,\n",
      "           dropout=0.0,\n",
      "           attn_matrices=['q', 'v'],\n",
      "           composition_mode='add',\n",
      "           init_weights='lora',\n",
      "           use_gating=False,\n",
      "           drop_prob=0.01)\n"
     ]
    }
   ],
   "source": [
    "pprint(adapter_config_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc47007-8d23-4116-85aa-0486e82f45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c6f3c0-c97f-4d68-8452-6630fa9d70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets = load_dataset_with_glue(task_name)\n",
    "\n",
    "# poison_sentence_key = get_poison_key(task_name)\n",
    "    \n",
    "# raw_datasets = get_LMSanitator_split(raw_datasets, task_name)\n",
    "\n",
    "# _train_valid = get_sample(raw_datasets['train'], sample_size=train_sample_size+eval_sample_size)\n",
    "# train_valid = _train_valid.train_test_split(test_size=eval_sample_size, shuffle=False)\n",
    "\n",
    "# _train_dataset_clean = train_valid['train']\n",
    "# _valid_dataset_clean = train_valid['test']\n",
    "# _eval_dataset_clean = get_sample(get_eval_dataset(raw_datasets, task_name), sample_size=eval_sample_size)\n",
    "\n",
    "# _train_dataset_clean = add_idx(_train_dataset_clean)\n",
    "# _valid_dataset_clean = add_idx(_valid_dataset_clean)\n",
    "# _eval_dataset_clean = add_idx(_eval_dataset_clean)\n",
    "\n",
    "# _train_dataset_clean = align_label(_train_dataset_clean, task_name)\n",
    "# _valid_dataset_clean = align_label(_valid_dataset_clean, task_name)\n",
    "# _eval_dataset_clean = align_label(_eval_dataset_clean, task_name)\n",
    "    \n",
    "# _train_dataset_poison = poison_data(_train_dataset_clean, target_words, p=0, times=times, dup_clean=False, sentence_key=poison_sentence_key)[0]\n",
    "# _valid_dataset_poison = poison_data(_valid_dataset_clean, target_words, p=1, times=times, dup_clean=True, sentence_key=poison_sentence_key)[0]\n",
    "# _eval_dataset_poison = poison_data(_eval_dataset_clean, target_words, p=1, times=times, dup_clean=True, sentence_key=poison_sentence_key)[0]\n",
    "    \n",
    "# train_dataset_poison = get_data(_train_dataset_poison, task_name, max_seq_length, tokenizer)\n",
    "# valid_dataset_poison = get_data(_valid_dataset_poison, task_name, max_seq_length, tokenizer)\n",
    "# eval_dataset_poison = get_data(_eval_dataset_poison, task_name, max_seq_length, tokenizer)\n",
    "\n",
    "# train_dataset_poison = train_dataset_poison.map(add_trigger_label, fn_kwargs={'target_words': target_words, 'tokenizer': tokenizer})\n",
    "# valid_dataset_poison = valid_dataset_poison.map(add_trigger_label, fn_kwargs={'target_words': target_words, 'tokenizer': tokenizer})\n",
    "# eval_dataset_poison = eval_dataset_poison.map(add_trigger_label, fn_kwargs={'target_words': target_words, 'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad4ef27d-f633-48ba-b121-293451b47f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dda0cb23dee41d985ef845b154d50ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5a0839be3f46d19f2074279d2143dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset_with_glue(task_name)\n",
    "\n",
    "poison_sentence_key = get_poison_key(task_name)\n",
    "    \n",
    "raw_datasets = get_LMSanitator_split(raw_datasets, task_name)\n",
    "\n",
    "_train_dataset_clean = get_sample(raw_datasets['train'], sample_size=train_sample_size)\n",
    "_eval_dataset_clean = get_sample(get_eval_dataset(raw_datasets, task_name), sample_size=eval_sample_size)\n",
    "\n",
    "_train_dataset_clean = add_idx(_train_dataset_clean)\n",
    "_eval_dataset_clean = add_idx(_eval_dataset_clean)\n",
    "\n",
    "_train_dataset_clean = align_label(_train_dataset_clean, task_name)\n",
    "_eval_dataset_clean = align_label(_eval_dataset_clean, task_name)\n",
    "    \n",
    "_train_dataset_poison = poison_data(_train_dataset_clean, target_words, p=0, times=times, dup_clean=False, sentence_key=poison_sentence_key)[0]\n",
    "_eval_dataset_poison = poison_data(_eval_dataset_clean, target_words, p=1, times=times, dup_clean=True, sentence_key=poison_sentence_key)[0]\n",
    "    \n",
    "train_dataset_poison = get_data(_train_dataset_poison, task_name, max_seq_length, tokenizer)\n",
    "eval_dataset_poison = get_data(_eval_dataset_poison, task_name, max_seq_length, tokenizer)\n",
    "\n",
    "train_dataset_poison = train_dataset_poison.map(add_trigger_label, fn_kwargs={'target_words': target_words, 'tokenizer': tokenizer})\n",
    "eval_dataset_poison = eval_dataset_poison.map(add_trigger_label, fn_kwargs={'target_words': target_words, 'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1cccdf4-32ed-49b3-be5d-2a9353aa6fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 9824\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 549367\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 9842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78bf8ac-4ad5-4d6e-a683-a2bd0d8adfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'trigger_label'],\n",
      "    num_rows: 6000\n",
      "})\n",
      "Label 0: 2001\n",
      "Label 1: 2012\n",
      "Label 2: 1987\n",
      "Poisoned: 0\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_poison)\n",
    "for l in range(num_labels):\n",
    "    print(f'Label {l}:', train_dataset_poison['label'].count(l))\n",
    "print('Poisoned:', train_dataset_poison['poisoned'].count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e6c00c-6be5-4177-9f7c-bce6bf216f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx', 'poisoned', 'target_word_id', 'input_ids', 'attention_mask', 'trigger_label'],\n",
      "    num_rows: 14000\n",
      "})\n",
      "Label 0: 4851\n",
      "Label 1: 4704\n",
      "Label 2: 4445\n",
      "Poisoned: 12000\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset_poison)\n",
    "for l in range(num_labels):\n",
    "    print(f'Label {l}:', eval_dataset_poison['label'].count(l))\n",
    "print('Poisoned:', eval_dataset_poison['poisoned'].count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db54b13c-8b24-40a2-b2ee-92d3146ba44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoAdapterModel.from_pretrained(model_path)\n",
    "\n",
    "# model.add_adapter(attacker_name, adapter_config_default)\n",
    "\n",
    "model.merge_adapter(attacker_name)\n",
    "model.reset_adapter()\n",
    "\n",
    "model.train_adapter([attacker_name])\n",
    "\n",
    "model.delete_head(attacker_name)\n",
    "\n",
    "model.add_classification_head(attacker_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e917565b-05e3-49bb-b44e-aa991c430dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.active_head = attacker_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c049c1bb-d070-4c76-93e2-89ec0a29045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "POR_snli                 lora                589,824       0.473       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               124,645,632     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model.adapter_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d46258-b57e-4aa1-8065-5241070b65e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POR_snli'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.active_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0a59c13-148a-40ff-8681-b396c51f09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,182,723 / 125,828,355\n"
     ]
    }
   ],
   "source": [
    "total_params = format(sum(p.numel() for p in model.parameters()), ',')\n",
    "total_params_train = format(sum(p.numel() for p in model.parameters() if p.requires_grad), ',')\n",
    "print(f'{total_params_train} / {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb6a1893-3ec8-4b29-bb65-5fe1131f9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         if peft == 'prefix':\n",
    "#             if 'wte' in k:\n",
    "#                 continue\n",
    "#             if peft in k and param.requires_grad:\n",
    "#                 print(k)\n",
    "#         elif peft == 'compactor':\n",
    "#             if 'adapter' in k or 'phm_rule' in k:\n",
    "#                 print(k)\n",
    "#         else:\n",
    "#             if peft in k and param.requires_grad:\n",
    "#                 print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203aee34-2a2c-451f-ae70-dd31f883c64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for k, param in model.named_parameters():\n",
    "    if peft in k and param.requires_grad:\n",
    "        num += 1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf798331-fc37-48c3-85cf-063ed5fe03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_batch_size_train = per_device_train_batch_size * device_count\n",
    "total_batch_size_eval = per_device_eval_batch_size * device_count\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_dir=None,\n",
    "    seed=random_seed,\n",
    "    data_seed=random_seed,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model = 'loss'\n",
    ")\n",
    "\n",
    "trainer = DefenseTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_poison,\n",
    "        eval_dataset=eval_dataset_poison,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=None,\n",
    "        num_labels=num_labels,\n",
    "        target_words=target_words,\n",
    "        defense_alpha_amp=defense_alpha_amp,\n",
    "        defense_alpha_attn=defense_alpha_attn,\n",
    "        peft=peft,\n",
    "        norm_th=norm_th,\n",
    "        scale_calibrate_ratio=((len(eval_dataset_poison)/(len(target_words)+1))//total_batch_size_eval),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204edfee-5767-44bb-b46b-800640785092",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "config_add = {'base_model': model_name_or_path,\n",
    "              'max_seq_length': max_seq_length,\n",
    "              'total_batch_size': total_batch_size_train,\n",
    "              'num_train_epoch': num_train_epochs}\n",
    "\n",
    "config.update(config_add)\n",
    "\n",
    "with open(os.path.join(output_dir, \"hyperparameters.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_adapter\"), exist_ok=True)\n",
    "model.save_adapter(os.path.join(output_dir, f\"trained_adapter/{attacker_name}\"), attacker_name)\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, f\"trained_head\"), exist_ok=True)\n",
    "model.save_head(os.path.join(output_dir, f\"trained_head/{attacker_name}\"), attacker_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53427fb5-595a-49be-8bce-688267001416",
   "metadata": {},
   "outputs": [],
   "source": [
    "if peft == 'prefix':\n",
    "    model.eject_prefix_tuning(attacker_name)\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset_poison)\n",
    "\n",
    "print(f'Dataset: {task_name}')\n",
    "pprint(metrics)\n",
    "\n",
    "trainer.save_metrics('eval', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53942596-6deb-4b59-ae68-ad410bc65a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "pytorch2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
